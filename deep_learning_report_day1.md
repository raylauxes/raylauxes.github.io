# 深層学習day1レポート（NN：ニューラルネットワーク）

![Image](/bnr_jdla.png)
http://study-ai.com/jdla/


### DL000_1_識別モデルと生成モデル

```
機械学習（深層学習）モデルを２つに分類できる：
1、識別モデル
2、生成モデル

識別モデルで、「x」が入力のデータである。
p(Ck|x)：k個のクラス（Class）のうち、xがそれぞれのクラスに当てる確率
```
![Image](/DL000_1_識別モデルと生成モデル_02m15s.png)

```
識別モデル vs. 生成モデル

※「隠れマルコフモデル」は自然言語処理に用いられる。例：「今日は」のうしろに「晴れ」が現れる確率
※VAE、GANは深層学習のモデル
※生成モデルの応用の１つ：画像の画質を良くする！
```
![Image](/DL000_1_識別モデルと生成モデル_06m17s.png)


### DL000_2_識別器の開発アプローチ

```
識別器（classifier）の開発には3つのアプローチがある：
1、生成モデル：確率的な識別
2、識別モデル：確率的な識別
3、識別関数：出力の結果が1か0となり、決定的な識別

※識別モデルと識別器の違いに要注意
```
![Image](/DL000_2_識別器の開発アプローチ_09m00s.png)


### DL000_3_識別器における生成モデル-識別モデル-識別関数-
```
左の図が生成モデルが学習した内容であり、右が識別モデルが学習した内容と考えられる
```
![Image](/DL000_3_識別器における生成モデル-識別モデル-識別関数-04m00s.png)

![Image](/DL000_3_識別器における生成モデル-識別モデル-識別関数-07m21s.png)
> Q: 事後確率とは？


### DL000_4_万能近似定理と深さ
```
なんで深層学習が流行っているのか？その理由は万能近似定理：
ニューラルネットワークはほぼどのような関数でも近似できる！
```
外部優良資料紹介：[Lecture 2 | The Universal Approximation Theorem](https://www.youtube.com/watch?v=lkha188L4Gs)


### DL001_ニューラルネットワークの全体像

```
入力のデータ（体長、体重、ひげの本数…）：[10, 300, 300, ...]
出力の結果（犬、猫、ネズミそれぞれである確率）：[0.1, 0.05, 0.85]

「学習」とは、Wとbの数字を探すこと。

※W：weight/重み（おもみ0）
※b：bias/バイアス（バイアス1）
```
![Image](/DL001_ニューラルネットワークの全体像_02m00s.png)

```
ディープラーニングはなにをしようとしているか？
「プログラム」（コード）の代わりに、「多数の中間層を持つニューラルネットワーク」を用いて、「入力値」から「求める出力値」に変換する数学モデルを構築すること
```
> ディープラーニングは、結局正解と一番近い出力値を再現させられるような、最適な「重み」と「バイアス」を探すこと

```
ニューラルネットワークで解決できる問題：
1、回帰：「連続する実数値」を取る関数の近似（身長や株価など連続するデータを予測）
回帰分析の手法：線形回帰、回帰木、ランダムフォレスト、ニューラルネットワーク（NN）

2、分類：性別や動物の種類（この写真が猫か）など「離散的な結果」を予想
分類分析の手法：ベイズ分類、ロジスティック回帰、決定木、ランダムフォレスト、ニューラルネットワーク
```

```
データを深層学習モデルで分析するため、数字に変換する必要がある。
例）翻訳：まず原文の文字を数字にして、その数字を入力し、得た結果の数字を文字（訳文）に変換。
例）音声解釈：音声→数字（入力）→数字（出力）→音声
例）囲碁：碁石の位置→数字（入力）→数字（出力）→碁石の位置
```


### DL002_入力層〜中間層 
> 実演リンク：[1_1_forward_propagation.ipynb](https://drive.google.com/file/d/1wxpJ-MtCGbbGH-qLg_dL4ei9IG1ImwxO/view?usp=sharing)
```
出力のzはまた次の層に入力として使う
```
![Image](/DL002_入力層〜中間層_03m30s.png)

```
W（重み）を変えると関数の傾きが変わるが、関数を上に移動させるため、b（バイアス）を加える必要があう
```
![Image](/DL002_入力層〜中間層_06m43s.png)

```
xを入力値として、三層の順伝播ネットワークをつくろ！
```
![Image](/1_1_forward_propagation.ipynb_3_layers.png)



### DL003_活性化関数
> 実演リンク：[1_1_forward_propagation.ipynb](https://drive.google.com/file/d/1wxpJ-MtCGbbGH-qLg_dL4ei9IG1ImwxO/view?usp=sharing)
```
活性化関数は非線形関数！（直線ではない！）
```
![Image](/DL003_活性化関数_02m00s.png)

```
中間層用と出力層用の活性化関数は違う。
```
![Image](/DL003_活性化関数_05m07s.png)

```
中間層用の関数：ReLU関数、シグモイド（ロジスティック）関数、ステップ関数
```
![Image](/functions.py_middle_layer_activation_fuctions.png)

```
1、ReLU関数：入力が0を超えたら、そのままを出力。0以下なら、0を出力
```
![Image](/DL003_活性化関数_08m27s.png)

```
2、シグモイド（ロジスティック）関数
```
![Image](/DL003_活性化関数_07m11s.png)

```
3、ステップ関数：入力が0を超えたら、1を出力。0以下なら、0を出力
```
![Image](/DL003_活性化関数_05m57s.png)



### DL004_出力層_誤差関数
```
Q：誤差を計算する際に、なぜ「引き算」でなく、「二乗」するのか？
A：引き算すると、正の誤差と負の誤差がある場合、結果は実際の誤差より小さい

Q：なぜ1/2する？
A：誤差関数には、計算上しやすくするため「1/2」がつけられた。具体的に、誤差逆伝播の計算で、誤差関数を微分する。その時、2乗の2が降りるから、1/2を用意したほうが計算しやすい。つまり本質的な意味はない。

※本来分類問題は、誤差関数にクロスエントロピー誤差を用いるが、ここでは説明しやすいため
```
![Image](/DL004_出力層_誤差関数_12m41s.png)


### DL005_出力層_活性化関数
```
```
![Image](/.png)

> 実演リンク：[link title](https://)


### 

```
```
![Image](/.png)

> 実演リンク：[link title](https://)


### 

```
```
![Image](/.png)

> 実演リンク：[link title](https://)


### 

```
```
![Image](/.png)

> 実演リンク：[link title](https://)


### 

```
```
![Image](/.png)

> 実演リンク：[link title](https://)
