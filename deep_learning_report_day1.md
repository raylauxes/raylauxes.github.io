# 深層学習day1レポート（NN：ニューラルネットワーク）

![Image](/bnr_jdla.png)
http://study-ai.com/jdla/


### DL000_1_識別モデルと生成モデル

```
機械学習（深層学習）モデルを２つに分類できる：
1、識別モデル
2、生成モデル

識別モデルで、「x」が入力のデータである。
p(Ck|x)：k個のクラス（Class）のうち、xがそれぞれのクラスに当てる確率
```
![Image](/DL000_1_識別モデルと生成モデル_02m15s.png)

```
識別モデル vs. 生成モデル

※「隠れマルコフモデル」は自然言語処理に用いられる。例：「今日は」のうしろに「晴れ」が現れる確率
※VAE、GANは深層学習のモデル
※生成モデルの応用の１つ：画像の画質を良くする！
```
![Image](/DL000_1_識別モデルと生成モデル_06m17s.png)


### DL000_2_識別器の開発アプローチ

```
識別器（classifier）の開発には3つのアプローチがある：
1、生成モデル：確率的な識別
2、識別モデル：確率的な識別
3、識別関数：出力の結果が1か0となり、決定的な識別

※識別モデルと識別器の違いに要注意
```
![Image](/DL000_2_識別器の開発アプローチ_09m00s.png)


### DL000_3_識別器における生成モデル-識別モデル-識別関数-
```
左の図が生成モデルが学習した内容であり、右が識別モデルが学習した内容と考えられる
```
![Image](/DL000_3_識別器における生成モデル-識別モデル-識別関数-04m00s.png)

![Image](/DL000_3_識別器における生成モデル-識別モデル-識別関数-07m21s.png)
> Q: 事後確率とは？


### DL000_4_万能近似定理と深さ
```
なんで深層学習が流行っているのか？その理由は万能近似定理：
ニューラルネットワークはほぼどのような関数でも近似できる！
```
外部優良資料紹介：[Lecture 2 | The Universal Approximation Theorem](https://www.youtube.com/watch?v=lkha188L4Gs)


### DL001_ニューラルネットワークの全体像

```
入力のデータ（体長、体重、ひげの本数…）：[10, 300, 300, ...]
出力の結果（犬、猫、ネズミそれぞれである確率）：[0.1, 0.05, 0.85]

「学習」とは、Wとbの数字を探すこと。

※W：weight/重み（おもみ0）
※b：bias/バイアス（バイアス1）
```
![Image](/DL001_ニューラルネットワークの全体像_02m00s.png)

```
ディープラーニングはなにをしようとしているか？
「プログラム」（コード）の代わりに、「多数の中間層を持つニューラルネットワーク」を用いて、「入力値」から「求める出力値」に変換する数学モデルを構築すること
```
> ディープラーニングは、結局正解と一番近い出力値を再現させられるような、最適な「重み」と「バイアス」を探すこと

```
ニューラルネットワークで解決できる問題：
1、回帰：「連続する実数値」を取る関数の近似（身長や株価など連続するデータを予測）
回帰分析の手法：線形回帰、回帰木、ランダムフォレスト、ニューラルネットワーク（NN）

2、分類：性別や動物の種類（この写真が猫か）など「離散的な結果」を予想
分類分析の手法：ベイズ分類、ロジスティック回帰、決定木、ランダムフォレスト、ニューラルネットワーク
```

```
データを深層学習モデルで分析するため、数字に変換する必要がある。
例）翻訳：まず原文の文字を数字にして、その数字を入力し、得た結果の数字を文字（訳文）に変換。
例）音声解釈：音声→数字（入力）→数字（出力）→音声
例）囲碁：碁石の位置→数字（入力）→数字（出力）→碁石の位置
```


### DL002_入力層〜中間層 
> 実演リンク：[1_1_forward_propagation.ipynb](https://drive.google.com/file/d/1wxpJ-MtCGbbGH-qLg_dL4ei9IG1ImwxO/view?usp=sharing)
```
出力のzはまた次の層に入力として使う
```
![Image](/DL002_入力層〜中間層_03m30s.png)

```
W（重み）を変えると関数の傾きが変わるが、関数を上に移動させるため、b（バイアス）を加える必要があう
```
![Image](/DL002_入力層〜中間層_06m43s.png)

```
xを入力値として、三層の順伝播ネットワークをつくろ！
```
![Image](/1_1_forward_propagation.ipynb_3_layers.png)



### DL003_活性化関数
> 実演リンク：[1_1_forward_propagation.ipynb](https://drive.google.com/file/d/1wxpJ-MtCGbbGH-qLg_dL4ei9IG1ImwxO/view?usp=sharing)
```
活性化関数は非線形関数！（直線ではない！）
```
![Image](/DL003_活性化関数_02m00s.png)

```
中間層用と出力層用の活性化関数は違う。
```
![Image](/DL003_活性化関数_05m07s.png)

```
中間層用の関数：ReLU関数、シグモイド（ロジスティック）関数、ステップ関数
```
![Image](/functions.py_middle_layer_activation_fuctions.png)

```
1、ReLU関数：入力が0を超えたら、そのままを出力。0以下なら、0を出力
```
![Image](/DL003_活性化関数_08m27s.png)

```
2、シグモイド（ロジスティック）関数
```
![Image](/DL003_活性化関数_07m11s.png)

```
3、ステップ関数：入力が0を超えたら、1を出力。0以下なら、0を出力
```
![Image](/DL003_活性化関数_05m57s.png)



### DL004_出力層_誤差関数
```
Q：誤差を計算する際に、なぜ「引き算」でなく、「二乗」するのか？
A：引き算すると、正の誤差と負の誤差がある場合、結果は実際の誤差より小さい

Q：なぜ1/2する？
A：誤差関数には、計算上しやすくするため「1/2」がつけられた。具体的に、誤差逆伝播の計算で、誤差関数を微分する。その時、2乗の2が降りるから、1/2を用意したほうが計算しやすい。つまり本質的な意味はない。

※本来分類問題は、誤差関数にクロスエントロピー誤差を用いるが、ここでは説明しやすいため
```
![Image](/DL004_出力層_誤差関数_12m41s.png)


### DL005_出力層_活性化関数
```
中間層と出力層の活性化関数には目的が違う：

[値の強弱]
中間層：閾値の前後で信号の強弱を調整
出力層：信号の大きさ（比率）はそのままに変換

[確率出力]
分類問題の場合、出力層の出力は0~1の範囲に限定し、総和を1とする必要がある
```

```
出力層用の活性化関数：
ソフトマックス関数
恒等写像：何もしない、そのままに出力
シグモイド関数（ロジスティック関数）
```
![Image](/DL005_出力層_活性化関数_05m30s.png)

```
シグモイド関数（ロジスティック関数）
```
![Image](/DL005_出力層_活性化関数_07m20s.png)

```
ソフトマックス関数：３つ以上のクラスの場合、出力を足したら1（つまり100%）となる

K：クラス数

本質の計算は最後の1行である「np.exp(x) / np.sum(np.exp(x))」だけ。
※「if x = ndim == 2:...」はこの関数を便利に使うため
※「x = x - np.max(x)」はプログラムを安定させるため
```
![Image](/DL005_出力層_活性化関数_08m36s.png)


```
交差エントロピー：汎用性の高い誤差関数！

本質的な部分は最後の「-np.sum(np.log(y[np.arange(batch_size), d] + 1e-7))」
```
![Image](/DL005_出力層_活性化関数_15m27s.png)
```
対数関数が0に落ちないように、「1e-7」を加える
```
![Image](/DL005_出力層_活性化関数_17m00s.png)


### DL006_勾配降下法
```
ニューラルネットワークは学習の過程でうまく重み（W）とバイアス（b）を調整する。
その調整の仕方は、誤差関数を最小にするWとbを見つける。
このWとbを見つけるのは、勾配降下法のミッションである。

※E：誤差（関数）
```
```
勾配降下法の学習率にはよく用いられるアルゴリズム：
Momentum
AdaGrad
Adadelta
Adam
```
> 実演リンク：[1_3_stochastic_gradient_descent.ipynb](https://drive.google.com/file/d/1kk-V9IHIyR4rNG-Li-Ih5aqeiYDekAhN/view?usp=sharing)


### DL007_確率的勾配降下法
```
勾配降下法：全サンプルの平均誤差
vs
確率的勾配降下法：ランダムに抽出したサンプルの誤差
  メリット：1、計算コストの軽減
  　　　　　2、局所極小解に収束するリスクの軽減
       　　3、オンライン学習（少しずつデータを与えて学習させるので、徐々にデータを用意しても良い）ができる
          ※「オンライン学習」は「バッチ学習」（一度にすべての学習データを使ってパラメータ更新を行うため、一気にデータを準備する必要がある）の真っ逆
```


### DL008_ミニバッチ勾配降下法
![Image](/DL008_ミニバッチ勾配降下法_09m00s.png)
```
エポック：同じデータセットを1回全部用いる
```
![Image](/DL008_ミニバッチ勾配降下法_10m10s.png)


### DL009_誤差勾配の計算
```
各パラメータ（Wの各値）それぞれ微分することに計算の負荷が大きいので、誤差逆伝播法を活用する！
```
![Image](/DL009_誤差勾配の計算_05m33s.png)


### DL010_誤差逆伝播法_誤差勾配の計算
```
右側（誤差）から微分すると、重複した計算を避けられる！
```
![Image](/DL010_誤差逆伝播法_誤差勾配の計算_06m10s.png)

```
# 平均二乗誤差の導関数
def d_mean_squared_error(d, y):
    if type(d) == np.ndarray:
        batch_size = d.shape[0]
        dx = (y - d)/batch_size
    else:
        dx = y - d
    return dx
```
![Image](/1_3_stochastic_gradient_descent.ipynb_backward.png)

```
W2を求める部分を説明すると
```
![Image](/DL010_誤差逆伝播法_誤差勾配の計算_19m09s.png)

![Image](/DL010_誤差逆伝播法_誤差勾配の計算_22m00s.png)
```
回答：
1、delta1 = np.dot(delta2, W2.T) * functions.d_sigmoid(z1)
2、grad['W1'] = np.dot(x.T, delta1)
```


### 
```
```
![Image](/.png)

> 実演リンク：[link title](https://)


### 
```
```
![Image](/.png)

> 実演リンク：[link title](https://)


### 
```
```
![Image](/.png)

> 実演リンク：[link title](https://)

