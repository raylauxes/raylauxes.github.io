# 深層学習day4レポート(強化学習)

![Image](/bnr_jdla.png)
http://study-ai.com/jdla/


### DL04-001_強化学習1
![Image](/DL04-001_強化学習1_00m40s.png)
```
強化学習とは、長期的に報酬を最大化できるように環境のなかで行動を選択できるエージェントを作ることを目標とする機械学習の一分野
```
![Image](/DL04-001_強化学習1_06m30s.png)


### DL04-002_強化学習2
```
強化学習の応用例：
誰にキャンペーンのメールを送ろうか？
激安品しか買わない顧客に送ると、損になる。キャンペーン商品とほかの商品を一緒に買ってくれそうなお客に送ると売り上げが上がる！
```


### DL04-003_強化学習3
```
「教師あり・なし学習」と「強化学習」の違い：目標が違う！

教師あり・なし学習では、データに含まれるパターン（特徴！）を見つけ出すおよびそのデータから予測することが目標

強化学習では、優れた方策を見つけることが目標
```
```
Q学習：行動価値関数を、行動する毎に更新することにより学習を進める方法

関数近似法：価値関数や方策関数を関数近似する手法のこと
```


### DL04-004_強化学習4
```
価値関数は２種ある：
1、状態価値関数
2、行動価値関数
```
```
方策関数（ほうさくかんすう）とは、エージェントが状況によるどう反応するかを決める関数。
価値関数を最大値（最大の報酬）を出すように、方策関数が動いている
```
![Image](/DL04-002_強化学習2_07m00s.png)


### DL04-005_強化学習5
```
方策勾配法：方策をモデル化して最適化する手法
θ：ニューラルネットワークのW（重み）と近い
E：学習率

Jとは、方策の良さ（定義しなきゃ！）
```
![Image](/DL04-005_強化学習5_04m15s.png)

```
π：方策関数
```
![Image](/DL04-005_強化学習5_08m15s.png)
![Image](/DL04-005_強化学習5_10m00s.png)
![Image](/DL04-005_強化学習5_11m40s.png)


### DL04-006_AlphaGo1
```
AlphaGo:
1. AlphaGo Lee
2. AlphaGo Zero
```

```
PolicyNet（方策関数）
```
![Image](/DL04-006_AlphaGo1_02m00s.png)
![Image](/DL04-006_AlphaGo1_04m15s.png)


```
ValueNet（価値関数）
```
![Image](/DL04-006_AlphaGo1_09m45s.png)


### 
```

```
![Image](/.png)
> 実演リンク：[title](https://)


### 
```

```
![Image](/.png)
> 実演リンク：[title](https://)


### 
```

```
![Image](/.png)
> 実演リンク：[title](https://)


### 
```

```
![Image](/.png)
> 実演リンク：[title](https://)


### 
```

```
![Image](/.png)
> 実演リンク：[title](https://)








